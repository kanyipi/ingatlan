---
title: "Termproject"
author: "Peter Kaiser"
date: "12/19/2021"
output:
  html_document:
    toc: true
    toc_float: true
    theme: yeti
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
# install.packages(c("rvest", "data.table", "jsonlite"))
# rm(list = ls())
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(plyr)
library(knitr)
library(kableExtra)
library(rvest)
library(data.table)
library(tidyverse)

get_one_property <- function(url) {
  t_list <- list()

  t <- read_html(url)

  t_list[["address"]] <- t %>%
    html_nodes(".address") %>%
    html_text() %>%
    trimws()

  t_list[["price"]] <- t %>%
    html_nodes(".parameterTitleLink+ .parameterValues span") %>%
    html_text()

  t_list[["area"]] <- t %>%
    html_nodes(".parameter:nth-child(2) span") %>%
    html_text()

  t_list[["noroom"]] <- t %>%
    html_nodes(".parameter~ .parameter+ .parameter .parameterValues , .restricted") %>%
    html_text()

  t_list[["description"]] <- t %>%
    html_nodes(".longDescription") %>%
    html_text()

  keys <- t %>%
    html_nodes(".parameterName") %>%
    html_text() %>%
    trimws()

  values <- t %>%
    html_nodes(".parameterValue") %>%
    html_text() %>%
    trimws()

  if (length(keys) == length(values)) {
    for (i in 1:length(keys)) {
      t_list[[keys[i]]] <- values[i]
    }
  }

  return(t_list)
}

# get_one_property gets the data points for one house namely:
# the address, the price, the size, the number of rooms,
# the description and the list data values e.g.: the year of construction, the condition and so on

# test
# url <- 'https://ingatlan.com/xvi-ker/elado+lakas/tegla-epitesu-lakas/32395911'
# get_one_house(url)

get_links_on_page <- function(url) {
  t <- read_html(url)

  rel_links <- t %>%
    html_nodes(".listing__link") %>%
    html_attr("href")

  links <- paste0("https://ingatlan.com", rel_links)

  return(links)
}

# get_links_on_page gets every relative link on a ingatlan.com search page
# and coverst them to an absolute link

# test
# url <- 'https://ingatlan.com/lista/elado+lakas+xvi-ker?page=2'
# get_links_on_page(url)


get_read_property <- function(nameofcsvs, linktoscrape, noofpagestoget = 0,
                              batchsize = 50, startbatchnumber = 0) {
  # nameofcsvs is the name of the folder and the csvs
  # linktoscrape is the search link to scrape,
  # noofpagestoget is how much pages we should get, if 0 or not set it will get all
  # batchsize is how many pages goes into a csv
  # startbatchnumber is to set the first batch, good for saving default=0

  # create the dir
  dir.create(nameofcsvs)

  # get the last page number
  lastpagetext <- read_html(linktoscrape) %>%
    html_nodes(".pagination__page-number") %>%
    html_text() %>%
    trimws()

  lastpageno <- lastpagetext %>%
    substr(2, nchar(lastpagetext)) %>%
    parse_number()

  # check whats smaller the last page or the noofpages to get
  if (noofpagestoget != 0) {
    if (noofpagestoget < lastpageno) {
      lastpageno <- noofpagestoget
    }
  }

  lastpagebatch <- floor(lastpageno / batchsize) + 1


  # iterate through the batches
  for (i in startbatchnumber:lastpagebatch) {
    print(paste("start", i))

    # si= start page index in the batch
    # ei= end page index in the batch
    si <- ((i * batchsize) + 1)
    ei <- (((i + 1) * batchsize))

    # check to see if ei is bigger than lastpageno
    if (lastpageno < ei) {
      ei <- lastpageno
    }

    # get all search pages in batch
    links_to_get_links <- paste0(linktoscrape, "?page=", si:ei)
    # get all property links from the search pages
    link_list <- sapply(links_to_get_links, get_links_on_page)

    # initiate
    data_list <- list()
    k <- 0

    # loop through pages with foor because I couldnt try catch a lapply
    for (j in link_list) {
      # try catch as http 503 was very common
      tryCatch(
        {
          oneproperty <- get_one_property(j)
          # only k if get_one_property(j) did not error
          k <- k + 1
        },
        error = function(e) {
          # show error and errored link
          print(e)
          print(j)
        }
      )
      # if list has data point insert it to data_list
      if (length(oneproperty) > 1) {
        data_list[[k]] <- oneproperty
      }
    }
    # rbind lists to data frame
    df <- rbindlist(data_list, fill = T)
    # save data frame
    csvname <- paste0(nameofcsvs, "/", nameofcsvs, i, ".csv")
    write.csv(df, csvname)
  }
}


# old read
# for (i in 1:34) {
#  links_to_get_links <- paste0("https://ingatlan.com/lista/elado+lakas+budapest+ar-szerint?page=", (i * 50):((i * 50) + 50))
#  link_list <- sapply(links_to_get_links, get_links_on_page)
#  data_list <- lapply(link_list, get_one_house)
#  df <- rbindlist(data_list, fill = T)
#  csvname <- paste0("budapest", i, ".csv")
#  write.csv(df, csvname)
# }



# read in the saved files given the name
get_all_property <- function(nameofcsv) {
  csv_list <- list.files(nameofcsv)
  max_file_no <- 0

  for (i in csv_list) {
    fileno <- parse_number(i)
    if (fileno > max_file_no) {
      max_file_no <- fileno
    }
  }

  houses <- read.csv(paste0(nameofcsv, "/", nameofcsv, "0.csv"))

  df <- subset(houses, select = -Panelprogram)

  for (i in 1:max_file_no) {
    houses <- read.csv(paste0(nameofcsv, "/", nameofcsv, i, ".csv"))
    if (ncol(houses) == 27) {
      houses <- subset(houses, select = -Panelprogram)
    }
    print(i)
    df <- rbind(df, houses)
  }
  return(df)
}

# get nth element from list
elementlist <- function(lst, n) {
  sapply(lst, `[`, n)
}

get_clean <- function(df) {

  # remove X as it got created when joining the subfiles
  df <- df %>% subset(select = -X)

  # check for unique values for description and area,
  # we need area as a couple of residental park propeties have the same
  # description, but different area
  df <- df %>%
    group_by(description, area) %>%
    slice(1)

  # convert the eur prices into huf prices using real time conversion rate
  conversiourl <- "https://www.xe.com/currencyconverter/convert/?Amount=1&From=EUR&To=HUF"
  t <- read_html(conversiourl)
  eur_to_huf <- t %>%
    html_nodes(".iGrAod") %>%
    html_text() %>%
    str_split(" ") %>%
    elementlist(1) %>%
    substr(1, 7) %>%
    as.double()

  # convert currencies to double
  df <- df %>% mutate(price_in_cur = elementlist(str_split(price, " "), 3))
  df <- df %>% mutate(price_in_cur = ifelse(price_in_cur != "Ft", "EUR",
    ifelse(price_in_cur == "Ft", "HUF", "other")
  ))
  df <- df %>% mutate(price = elementlist(str_split(price, " "), 1))
  df <- df %>% mutate(price = as.double(str_replace(price, ",", ".")))
  df <- df %>% mutate(price = as.double(ifelse(price_in_cur == "EUR", price * eur_to_huf / 1000000, price)))

  # convert the area into integer
  df <- df %>% mutate(area = (as.numeric(elementlist(str_split(area, " "), 1))))

  # convert number of rooms to double
  # filtering as there are some properties where there are no full rooms, or wrong data such as 540 m
  # this removes 34 observations
  df <- df %>% filter(length(str_split(noroom, " ")[[1]]) != 2)

  df <- df %>% mutate(nohalfroom = ifelse(length(str_split(noroom, "\\+")[[1]]) > 1, parse_integer(elementlist(str_split(noroom, " "), 3)), 0))
  df <- df %>% mutate(noroom = as.numeric(elementlist(str_split(noroom, " "), 1)))


  df <- df %>% mutate(noroom = as.double(noroom + (nohalfroom / 2)))

  return(df)
}

# some example runs

# scrapename <- "hungary"
# get_read_property(scrapename,"https://ingatlan.com/lista/elado+lakas")
# all hungarian properties on ingatlan.com

# scrapename <- "fiftycheapbud"
# get_read_property(scrapename,"https://ingatlan.com/szukites/elado+lakas+budapest+ar-szerint",50,25)
# first 50 pages of the cheapest properties in budapest

# scrapename <- "miskolc"
# get_read_property(scrapename,"https://ingatlan.com/szukites/elado+lakas+miskolc")
# all Miskolc properties on ingatlan.com

# some good naming convention would be to name them as hungary20211210
# for the date of the scrape but that would mess up my read back
# so maybe a better solution would be to tag the properties when read in with Sys.time()

## HUNGARY ALL SCRAPE
# scrapename <- "hungary"
# get_read_property(scrapename, "https://ingatlan.com/lista/elado+lakas+budapest")
# df <- get_all_property(scrapename)
# df <- df %>% get_clean()
# write.csv(df,"hungaryall.csv")


## READ IN CLEANED DATA
df <- read.csv("https://raw.githubusercontent.com/kanyipi/ingatlan/main/hungary/hungaryall.csv")


# some upgrades which could be done:
# update the last page while running, as it may change
# update the reading back code so files work with numbers
# tag the data, with the link it cam from and the date
# implement some search
```

## Introduction

This project began as a full ingatlan.com scrape but it turned into a functional ingatlan.com scrapper. One can give it a search link and the number of pages they want to scrape and it will scrape every property on those pages.

## What it does

The major functions are:

get_one_property which scrapes a property from an ingatlan.com link

get_links_on_page which returns the absolute links from an ingatlan.com search page

get_all_property which does the scraping of the given link and saves down the properties in batches

get_read_property which reads back the csvs to a data frame

get_clean which cleans the data

## Analysis

Here is the structure of the data in a table. I only included the first twenty characters of the description, as they get very long and it would cause issues with the HTML.
```{r}
df %>%
  mutate(description = substr(description, 1, 20)) %>%
  head() %>%
  kable()
```

On the first histogram we can see that the price distribution has it's majority between 0 and 100, and most of the properties there are rounded to 30 m2, 60 m2, and 90 m2.

```{r}
df %>%
  mutate(area = as.factor(round_any(area, 30))) %>%
  ggplot(aes(x = price, fill = area)) +
  geom_histogram(binwidth = 10) +
  xlim(0, 300) +
  labs(
    x = "Price (in Millions HUF)",
    y = "Count",
    title = "Price Histogram of Hungarian Properties on Sale"
  ) +
  theme_bw()
```

On this histogram we can see that the size distribution has it's majority between 20 and 100, and most of the properties there are rounded to 30 m HUF, 60 m HUF, and 90 m HUF.

```{r}
df %>%
  mutate(price = as.factor(round_any(price, 30))) %>%
  ggplot(aes(x = area, fill = price)) +
  geom_histogram(binwidth = 10) +
  xlim(0, 300) +
  labs(
    x = "Size (in m2)",
    y = "Count",
    title = "Size Histogram of Hungarian Properties on Sale"
  ) +
  theme_bw()
```

On this violin plot we can see that the least properties are in "befejezetlen" which means it is not finished and that this category has the widest confidence interval. The average price for the not finished ("befejezetlen"), the newly built ("új építésu"), and the close to newly built ("újszeru") are pretty similar, with newly built being the highest.

```{r}
df %>%
  filter(!is.na(Ingatlan.állapota) & !is.na(price)) %>%
  mutate(Ingatlan.állapota = as.factor(Ingatlan.állapota)) %>%
  ggplot(aes(x = Ingatlan.állapota, y = price, colour = Ingatlan.állapota)) +
  ylim(0, 300) +
  geom_violin() +
  geom_jitter(alpha = .2, width = .1) +
  geom_boxplot(alpha = .5) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  guides(colour = "none") +
  labs(
    x = "Property Condition",
    y = "Price (in Millions HUF)",
    title = "Violin Plot of Prices in Different Property Conditions"
  )
```

On this violin plot we can see an interesting trend, namely that the mean price of the properties built before 1950 is way higher than those of that were built later, between 1950 and 2000 (these fall within two categories: 1950-1980 and 1981-2000). Than the houses built between 2001 and 2010 is higher in price. We can also see that these categories are denser as these are intervals instead of years. Then from 2011 until 2022 the mean is not changing too much. Then the price goes up for 2023, 2024, and 2025. My theory is that only expensive properties are planned and advertised multiple years in advance, and cheaper properties are only advertised once they are built. NA ("nincs megadva") has the lowest mean, this may mean that they do not know when it was built, or they just do not want to tell.


```{r}
df %>%
  filter(!is.na(Építés.éve) & !is.na(price)) %>%
  mutate(Építés.éve = as.factor(Építés.éve)) %>%
  ggplot(aes(x = Építés.éve, y = price, colour = Építés.éve)) +
  ylim(0, 300) +
  geom_violin() +
  geom_jitter(alpha = .2, width = .1) +
  geom_boxplot(alpha = .5) +
  scale_x_discrete(guide = guide_axis(n.dodge = 3)) +
  guides(colour = "none") +
  labs(
    x = "Year Built",
    y = "Price (in Millions HUF)",
    title = "Violin Plot of Prices in Different Different Building Years"
  )
```

## Conclusion

In conclusion this is a very nice tool to scrape ingatlan.com as you wish. One can customize the inputs in a variety of ways, from naming the files, through the search link to use, to the batch size, and until number of properties to get. About the data, the price, and the size of properties have a very similar density shape with a right long tail. Lastly we could see that for the building conditions, it is usually true that the better the condition, the higher the price. The same was not true for building years as we could see a dip in the interval from 1950 to 2000, and a positive trend after that until 2024. 
