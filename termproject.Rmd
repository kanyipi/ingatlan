---
title: "Termproject"
output: 
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
# Set graph size
# knitr::opts_chunk$set(echo = FALSE, out.width = "50%" )#fig.asp = 0.5, fig.width = 7, out.width = "90%" )

rm(list = ls())

# Libraries

library(AER)
library(tidyverse)
library(lspline)
library(fixest)
library(modelsummary)
library(ggpubr)
library(reshape2)
library(kableExtra)
library(rvest)

# Get the data

df <- read.csv("https://raw.githubusercontent.com/kanyipi/ingatlan/main/budapestall/budapestallfull.csv")
```

## Introduction

This is a causal analysis on the properties on sale in Budapest. The data was scraped from ingatlan.com by me on the 16th of December, 2021. I investigate the effect of size of the property on the prize of the property. 

HERE COMES THE MOTIVATION WHY THIS IS A MEANINGFUL PROJECT AND WHAT IS THE MAIN GOAL! TODO


## Data

The data is from ingatlan.com. Scraped on the 16th of December, 2021. The scrapper went through every property listed under https://ingatlan.com/szukites/elado+lakas+budapest. It scraped, the address, the price, the number of rooms, the description, and the 20 element long list below the description. We have 27908 observation and 27 variables.
TODO appendixbe


```{r, echo=FALSE}

# Cleaning and data summary

# Helper function to get the nth element of a list
elementlist <- function(lst, n) {
  sapply(lst, `[`, n)
}

# Cleaning function
get_clean <- function(df) {

  # remove X as it got created when joining the subfiles
  df <- df %>% subset(select = -X)

  # check for unique values for description and area,
  # we need area as a couple of residental park propeties have the same
  # description, but different area
  df <- df %>%
    group_by(description, area) %>%
    slice(1)

  # convert the eur prices into huf prices using real time conversion rate
  conversiourl <- "https://www.xe.com/currencyconverter/convert/?Amount=1&From=EUR&To=HUF"
  t <- read_html(conversiourl)
  eur_to_huf <- t %>%
    html_nodes(".iGrAod") %>%
    html_text() %>%
    str_split(" ") %>%
    elementlist(1) %>%
    substr(1, 7) %>%
    as.double()
  df <- df %>% mutate(price = as.double(ifelse(price_in_cur == "EUR", price * eur_to_huf / 1000000, price)))

  # convert the area into integer
  df <- df %>% mutate(area = (as.numeric(elementlist(str_split(area, " "), 1))))

  # convert number of rooms to double
  # filtering as there are some properties where there are no full rooms, or wrong data such as 540 m
  # this removes 34 observations
  df <- df %>% filter(length(str_split(noroom, " ")[[1]]) != 2)

  df <- df %>% mutate(nohalfroom = ifelse(length(str_split(noroom, "\\+")[[1]]) > 1, parse_integer(elementlist(str_split(noroom, " "), 3)), 0))
  df <- df %>% mutate(noroom = as.numeric(elementlist(str_split(noroom, " "), 1)))


  df <- df %>% mutate(noroom = as.double(noroom + (nohalfroom / 2)))

  # convert condition to integer
  df <- df %>%
    mutate(condition = dplyr::recode(Ingatlan.állapota, "nincs megadva" = "NA", "befejezetlen" = "0", "felújítandó" = "1", "közepes állapotú" = "2", "jó állapotú" = "3", "felújított" = "4", "újszeru" = "5", "új építésu" = "6")) %>%
    mutate(condition = as.numeric(condition))

  # convert year built to integer
  df <- df %>%
    mutate(yearbuilt = dplyr::recode(Építés.éve, "nincs megadva" = "NA", "2001 és 2010 között" = "2010", "1950 elott" = "1950", "1950 és 1980 között" = "1980", "1981 és 2000 közöt" = "2000")) %>%
    mutate(yearbuilt = as.numeric(yearbuilt))

  return(df)
}

# filtering functions
get_filtered <- function(df, cols, price_filter_1 = 5, price_filter_2 = 300, area_filter_1 = 10, area_filter_2 = 250) {

  # start n = 27874

  # select needed columns drop observation if something is na
  df <- df %>%
    select(all_of(cols)) %>%
    drop_na()

  # n 18721

  df <- df %>% filter(price_filter_1 < price && price < price_filter_2)

  # n 18387

  df <- df %>% filter(area_filter_1 < area && area < area_filter_2)

  # n 18343

  return(df)
}

filter_cols <- c(
  "price", "address", "area",
  "noroom", "yearbuilt", "condition"
)

df <- df %>% get_clean()
df <- df %>% get_filtered(filter_cols)


P95 <- function(x) {
  quantile(x, 0.95, na.rm = T)
}
P05 <- function(x) {
  quantile(x, 0.05, na.rm = T)
}
datasummary((`Price` <- price) +
  (`Size` <- area) +
  (`Number of rooms` <- noroom) +
  (`Building year` <- yearbuilt) +
  (`Condition` <- condition) ~
Mean + Median + SD + Min + Max + P05 + P95,
data = df,
title = "Descriptive statistics"
) %>%
  kable_styling(latex_options = c("HOLD_position", "scale_down"))
```

The number of observations is `r sum(!is.na(df$price))` for all of our key variables.

DESCRIPTION OF THE SUMMARY STATS: WHAT CAN WE LEARN FROM THEM? TODO

As the focus is the price difference, the next Figure shows the histogram for this variable.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }
# price
p1 <- ggplot(df, aes(x = price)) +
  geom_histogram(binwidth = 10, fill = "navyblue", color = "white") +
  labs(y = "Count", x = "Price of properties on sale in Budapest in million of HUF") +
  theme_bw()

# area
p2 <- ggplot(df, aes(x = area)) +
  geom_histogram(binwidth = 10, fill = "navyblue", color = "white") +
  labs(y = "Count", x = "Size of properties on sale in Budapest in m2") +
  theme_bw()

association_figs <- ggarrange(p1, p2,
  hjust = -0.6,
  ncol = 2, nrow = 1
)
association_figs
# TODO
```

DESCRIPTION OF THE FIGURE. WHAT DOES IT TELS US?

(May change the order of descriptive stats and graph.)

The key pattern of association is:

```{r, echo=FALSE, warning=FALSE, fig.width=4, fig.height = 3, fig.align="center" }
chck_sp_normal <- function(x_var, x_lab) {
  ggplot(df, aes(x = x_var, y = price)) +
    geom_point(color = "red", size = 2, alpha = 0.6) +
    geom_smooth(method = "loess", formula = y ~ x) +
    labs(x = x_lab, y = "Average prices") +
    theme_bw()
}

chck_sp_log <- function(x_var, x_lab) {
  ggplot(df, aes(x = x_var, y = logprice)) +
    geom_point(color = "red", size = 2, alpha = 0.6) +
    geom_smooth(method = "loess", formula = y ~ x) +
    labs(x = x_lab, y = "Average prices") +
    theme_bw()
}

df <- df %>% mutate(logprice = log(price))
df <- df %>% mutate(logarea = log(area))


# Our main interest: student-to-teacher ratio:
p3 <- chck_sp_normal(df$area, "Size")
p4 <- chck_sp_normal(df$logarea, "Log size")
p5 <- chck_sp_log(df$area, "Size")
p6 <- chck_sp_log(df$logarea, "Log size")

association_figs2 <- ggarrange(p3, p4, p5, p6,
  ncol = 2, nrow = 2
)
association_figs2

chck_sp_log(df$condition, "condition")
# TODO
```

How will you include this in your model?
TODO
Short description on the other variables: 2-10 sentence depends on the amount of variables you have. You should reference your decisions on the graphs/analysis which are located in the appendix.

## Model


```{r, echo = FALSE }
# TODO
# reg1: NO control, simple linear regression
reg1 <- feols(logprice ~ logarea, data = df, vcov = "hetero")

# reg2: NO controls, use piecewise linear spline(P.L.S) with a knot at 18
reg2 <- feols(logprice ~ lspline(logarea, 4.5), data = df, vcov = "hetero")

# reg3: control for english learners dummy (english_d) only.
#   Is your parameter different? Is it a confounder?

reg3 <- feols(logprice ~ lspline(logarea, 4.5) + yearbuilt, data = df, vcov = "hetero")
##
# reg4: reg3 + Schools' special students measures (lunch with P.L.S, knot: 15; and special)

reg4 <- feols(logprice ~ lspline(logarea, 4.5) + yearbuilt +
  +lspline(noroom, 4), data = df, vcov = "hetero")

#
# reg5: reg4 + salary with P.L.S, knots at 35 and 40, exptot, log of income and scratio

reg5 <- feols(logprice ~ lspline(logarea, 4.5) + lspline(yearbuilt, 1990) +
  +lspline(noroom, 4) + condition, data = df, vcov = "hetero")

# Naming the coefficients for pretty output
# TODO
# alpha  <- round( reg5$coeftable[1,1] , 2 )
# b1 <- round( reg5$coeftable[2,1] , 2 )
# b2 <- round( reg5$coeftable[3,1] , 2 )
```
TODO
My preferred model is:

score = $`r alpha`$ $`r b1`$ $( student/teacher < 18)$ $`r b2`$ $( student/teacher \geq 18) + \delta Z$

where $Z$ are standing for the controls, which includes controlling for english language, lunch, other special characteristics and wealth measures. From this model we can infer:

- when every covariates are zero, students expected to have grade score of $`r alpha`$
- when the student to teacher is one unit larger, but below the value of 18, we see students to have on average $`r abs(b1)`$ smaller grades.
- when the student to teacher is one unit larger, with the value above or equal to 18, we see students to have on average $`r abs(b2)`$ smaller grades.

However, based on the heteroskedastic robust standard errors, these results are statistically non different from zero. To show that, I have run a two-sided hypothesis test:
$$H_0:=\beta_1 = 0$$
$$H_A:=\beta_1 \neq 0$$
I have the t-statistic as `r round( reg5$coeftable[2,3] , 2 )` and the p-value as `r round( reg5$coeftable[2,4] , 2 )`, which confirms my conclusion.

We compare multiple models to learn about the stability of the parameters. Bla-bla:

```{r, echo = FALSE }
##
# Summarize our findings:
# varname_report <- c("(Intercept)" = "Intercept",
#                    "stratio" = "student/teacher",
#                    "lspline(stratio,18)1" = "student/teacher (<18)",
#                    "lspline(stratio,18)2" = "student/teacher (>=18)",
#                    "english_d" = "english_dummy")
# groupConf <- list("English" = c("english"),
#                   "Lunch" = c("lunch"),
#               "Other Special" = c("special"),
#               "Wealth Measures" = c("exptot","income","scratio"))
# vars_omit <- c("english|lunch|special|salary|exptot|income|scratio")
#
# # Note: coefstat = 'confint' is just an example, usually you need to report se.
# style_noHeaders = style.tex(var.title = "", fixef.title = "", stats.title = " ")
#
#
# kable( etable( reg1 , reg2 , reg3 , reg4 , reg5 ,
#         title = 'Average test scores for 4th graders',
#         dict = varname_report,
#         drop = vars_omit ,
#         group = groupConf ,
#         se.below = T,
#         coefstat = 'se',
#         fitstat = c('n','r2'),
#         se.row = F,
#         depvar = F ) ,
#         col.names = c('(1)','(2)','(3)','(4)','(5)'),
#        "latex", booktabs = TRUE,  position = "H",
#        caption = 'Models to uncover relation between test score and student to teacher ratio') %>% kable_styling(latex_options = c("hold_position","scale_down"))
```





## Robustness check / 'Heterogeneity analysis'

Task: calculate and report t-tests for each countries. 
TODO


## Conclusion
TODO
HERE COMES WHAT WE HAVE LEARNED AND WHAT WOULD STRENGHTEN AND WEAKEN OUR ANALYSIS.

## Appendix
TODO
Here comes all the results which are referenced and not essential for understanding the MAIN results.
