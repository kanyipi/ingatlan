---
title: "Termproject"
output: 
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
#TODO
#• Data is well described, with sources, important features. May be (not needed) graph(s), table(s) to
#support. 15%
#• Modelling choices are well presented, and supported by
#Results are logically presented. Key coefficient(s) explained precisely. Results are appropriately interpreted. 20%(interpret other coeffs)
#robustness
#extra live euro conversion
#map

rm(list = ls())

# Libraries

library(AER)
library(tidyverse)
library(lspline)
library(fixest)
library(modelsummary)
library(ggpubr)
library(reshape2)
library(kableExtra)
library(rvest)

# Get the data

df <- read.csv("https://raw.githubusercontent.com/kanyipi/ingatlan/main/budapestall/budapestallfull.csv")
```

## Introduction

This is a causal analysis on the properties on sale in Budapest. The data was scraped from ingatlan.com by me on the 16th of December, 2021. I investigate the effect of the size of the property on the price of the property. 

I think analyzing housing prices is always meaningful, as it is a crucial part of our daily life, as everyone needs to live somewhere. People buy sell and rent properties all the time, which is influenced by the prices of the properties. Understanding the change in the prices offers us the opportunity to have further investigations.
My research question is simple:
Are the properties on sale in Budapest on average more expensive if they are larger?

## Data

The data is from ingatlan.com. It was scraped on the 16th of December, 2021. The scrapper went through every property listed under https://ingatlan.com/szukites/elado+lakas+budapest. It scraped, the address, the price, the number of rooms, the description, and the 20 element long list below the description. We have 27908 observations and 27 variables.
The scrape and the cleaning are more detailed in the appendix.
TODO appendixbe kep es a scraper es clean filter.


```{r, echo=FALSE}

# Cleaning and data summary

# Helper function to get the nth element of a list
elementlist <- function(lst, n) {
  sapply(lst, `[`, n)
}

# Cleaning function
get_clean <- function(df) {

  # remove X as it got created when joining the subfiles
  df <- df %>% subset(select = -X)

  # check for unique values for description and area,
  # we need area as a couple of residental park propeties have the same
  # description, but different area
  df <- df %>%
    group_by(description, area) %>%
    slice(1)

  # convert the eur prices into huf prices using real time conversion rate
  conversiourl <- "https://www.xe.com/currencyconverter/convert/?Amount=1&From=EUR&To=HUF"
  t <- read_html(conversiourl)
  eur_to_huf <- t %>%
    html_nodes(".iGrAod") %>%
    html_text() %>%
    str_split(" ") %>%
    elementlist(1) %>%
    substr(1, 7) %>%
    as.double()
  df <- df %>% mutate(price = as.double(ifelse(price_in_cur == "EUR", price * eur_to_huf / 1000000, price)))

  # convert the area into integer
  df <- df %>% mutate(area = (as.numeric(elementlist(str_split(area, " "), 1))))

  # convert number of rooms to double
  # filtering as there are some properties where there are no full rooms, or wrong data such as 540 m
  # this removes 34 observations
  df <- df %>% filter(length(str_split(noroom, " ")[[1]]) != 2)

  df <- df %>% mutate(nohalfroom = ifelse(length(str_split(noroom, "\\+")[[1]]) > 1, parse_integer(elementlist(str_split(noroom, " "), 3)), 0))
  df <- df %>% mutate(noroom = as.numeric(elementlist(str_split(noroom, " "), 1)))


  df <- df %>% mutate(noroom = as.double(noroom + (nohalfroom / 2)))

  # convert condition to integer
  df <- df %>%
    mutate(condition = dplyr::recode(Ingatlan.állapota, "nincs megadva" = "NA", "befejezetlen" = "0", "felújítandó" = "1", "közepes állapotú" = "2", "jó állapotú" = "3", "felújított" = "4", "újszeru" = "5", "új építésu" = "6")) %>%
    mutate(condition = as.numeric(condition))

  # convert year built to integer
  df <- df %>%
    mutate(yearbuilt = dplyr::recode(Építés.éve, "nincs megadva" = "NA", "2001 és 2010 között" = "2010", "1950 elott" = "1950", "1950 és 1980 között" = "1980", "1981 és 2000 közöt" = "2000")) %>%
    mutate(yearbuilt = as.numeric(yearbuilt))

  return(df)
}

# filtering functions
get_filtered <- function(df, cols, price_filter_1 = 5, price_filter_2 = 300, area_filter_1 = 10, area_filter_2 = 250) {

  # start n = 27874

  # select needed columns drop observation if something is na
  df <- df %>%
    select(all_of(cols)) %>%
    drop_na()

  # n 18721
  
  # filtering on prices
  df <- df %>% filter(price_filter_1 < price && price < price_filter_2)

  # n 18387

  # filtering on sizes
  df <- df %>% filter(area_filter_1 < area && area < area_filter_2)

  # n 18343

  return(df)
}

filter_cols <- c(
  "price", "address", "area",
  "noroom", "yearbuilt", "condition"
)

df <- df %>% get_clean() %>% get_filtered(filter_cols)

# Creating log values for price and size
df <- df %>% mutate(logprice = log(price))
df <- df %>% mutate(logarea = log(area))

# helper functions
P95 <- function(x) {
  quantile(x, 0.95, na.rm = T)
}

P05 <- function(x) {
  quantile(x, 0.05, na.rm = T)
}

#data summary
datasummary((`Price` = price) +
  (`Size` = area) +
  (`Number of Rooms` = noroom) +
  (`Building Year` = yearbuilt) +
  (`Condition` = condition) ~
Mean + Median + SD + Min + Max + P05 + P95,
data = df,
title = "Descriptive statistics"
) %>%
  kable_styling(latex_options = c("HOLD_position", "scale_down"))
```

The number of observations is `r sum(!is.na(df$price))` for all of our key variables. These are after filtering on price so that it is greater than 5M HUF, but less then 300M HUF. Also filtered on size so it is less greater than 10 m2, but less than 250 m2. I choose these values as I feel like these are filtering for habitable houses. Usually bigger and more expensive properties were multiple houses, for investing. While smaller and cheaper properties were not really houses, but part of houses and people looking for swap houses.

These are the summary stats of the already filtered dataset, so as we can see the minimums and maximum are close to the filter values.
The mean of the price is 65.12M HUF the median is 54.5M HUF this means we have a long right tail.
The mean of the Size is 69.60 m2 the median is 64.0 m2 this means we have a long right tail.

As the focus are on the price and on the size, the next Figure shows the histograms for these variables.
I show the rest of the histograms in the appendix.
TODO
```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }
# price
p1 <- ggplot(df, aes(x = price)) +
  geom_histogram(binwidth = 10, fill = "navyblue", color = "white") +
  labs(y = "Count", x = "Price of Properties on Sale in Budapest in Millions of HUF") +
  theme_bw()

# size
p2 <- ggplot(df, aes(x = area)) +
  geom_histogram(binwidth = 10, fill = "navyblue", color = "white") +
  labs(y = "Count", x = "Size of Properties on Sale in Budapest in m2") +
  theme_bw()

association_figs <- ggarrange(p1, p2,
  hjust = -0.6,
  ncol = 2, nrow = 1
)
association_figs
```

We can see the long right tail on the histograms.

We can see the level, and log associations of price and area.
The key pattern of associations are:

```{r, echo=FALSE, warning=FALSE, fig.width=4, fig.height = 3, fig.align="center" }


chck_sp_normal <- function(df,x_var, x_lab="", y_lab="") {
  varlist <- c("price", x_var)
  df2 <- df %>%
    group_by(!!!rlang::syms(varlist)) %>%
    mutate(weight = n()) %>%
    mutate(weight_2 = (weight / 1000))
  ggplot(df2, aes(x = (!!!rlang::syms(x_var)), y = price)) +
    geom_point(color = "red", size = 2, alpha = 0.6) +
    geom_smooth(method = "loess", formula = y ~ x) +
    labs(x = x_lab, y = y_lab) +
    theme_bw()
}

chck_sp_log <- function(df,x_var, x_lab="", y_lab="") {
  varlist <- c("logprice", x_var)
  df2 <- df %>%
    group_by(!!!rlang::syms(varlist)) %>%
    mutate(weight = n()) %>%
    mutate(weight_2 = (weight / 1000))
  ggplot(df2, aes(x = (!!!rlang::syms(x_var)), y = logprice)) +
    geom_point(color = "red", size = 2, alpha = 0.6) +
    geom_smooth(method = "loess", formula = y ~ x) +
    labs(x = x_lab, y = y_lab) +
    theme_bw()
}


# Our main interest: student-to-teacher ratio:
p3 <- chck_sp_normal(df,"area", "", "Level Price")
p4 <- chck_sp_normal(df,"logarea")
p5 <- chck_sp_log(df,"area", " Level Size", "Log Price")
p6 <- chck_sp_log(df,"logarea", "Log Size")

association_figs2 <- ggarrange(p3, p4, p5, p6,
  ncol = 2, nrow = 2
)
association_figs2

# TODO
```

How will you include this in your model?
TODO
Short description on the other variables: 2-10 sentence depends on the amount of variables you have. You should reference your decisions on the graphs/analysis which are located in the appendix.

## Model


```{r, echo = FALSE }
# TODO
# reg1: NO control, simple linear regression
reg1 <- feols(logprice ~ logarea, data = df, vcov = "hetero")

# reg2: NO controls, use piecewise linear spline(P.L.S) with a knot at 18
reg2 <- feols(logprice ~ lspline(logarea, 4.5), data = df, vcov = "hetero")

# reg3: control for english learners dummy (english_d) only.
#   Is your parameter different? Is it a confounder?

reg3 <- feols(logprice ~ lspline(logarea, 4.5) + yearbuilt, data = df, vcov = "hetero")
##
# reg4: reg3 + Schools' special students measures (lunch with P.L.S, knot: 15; and special)

reg4 <- feols(logprice ~ lspline(logarea, 4.5) + yearbuilt +
  +lspline(noroom, 4), data = df, vcov = "hetero")

#
# reg5: reg4 + salary with P.L.S, knots at 35 and 40, exptot, log of income and scratio

reg5 <- feols(logprice ~ lspline(logarea, 4.5) + lspline(yearbuilt, 1990) +
  +lspline(noroom, 4) + condition, data = df, vcov = "hetero")

# Naming the coefficients for pretty output
# TODO
 alpha  <- round( reg5$coeftable[1,1] , 2 )
 b1 <- round( reg5$coeftable[2,1] , 2 )
 b2 <- round( reg5$coeftable[3,1] , 2 )
```
TODO
My preferred model is:

log(price)  = $`r alpha`$ + $`r b1`$ $( log(size) < 4,5)$ $`r b2`$ $( log area \geq 18) + \delta Z$

where $Z$ are standing for the controls, which includes controlling for year built, number of rooms, and condition. 
From this model we can infer:

- alpa $`r alpha`$ is hard to interpret as both sides are in log, so we are more interested in the differences
- when the size of properties is 1 percent larger, but below the log value of 4.5, we see properties to cost on average $`r abs(b1)`$ percent more.
- when the size of properties is 1 percent larger, but the log value is above or equal to 4.5, we see properties to cost on average $`r abs(b2)`$ percent more.

Based on the heteroskedastic robust standard errors, these results are statistically different from zero. To show that, I have run a two-sided hypothesis test:
$$H_0:=\beta_1 = 0$$
$$H_A:=\beta_1 \neq 0$$
I have the t-statistic as `r round( reg5$coeftable[2,3] , 2 )` and the p-value as `r round( reg5$coeftable[2,4] , 2 )`, which confirms my conclusion.

We compare multiple models to learn about the stability of the parameters:

```{r, echo = FALSE }
#
#Summarize our findings:
varname_report <- c("(Intercept)" = "Intercept",
                   "logarea" = "Log Size",
                   "lspline(logarea,4.5)1" = "Log Size (<4.5)",
                   "lspline(logarea,4.5)2" = "Log Size (>=4.5)",
                   "yearbuilt" = "Building Year",
                   "lspline(noroom,4)1" ="Number of Rooms < 4",
                   "lspline(noroom,4)2" ="Number of Rooms >= 4",
                   "lspline(yearbuilt,1990)1" = "Building Year <1990",
                   "lspline(yearbuilt,1990)2" = "Building Year >=1990",
                   "condition" = "Condition")

# Note: coefstat = 'confint' is just an example, usually you need to report se.
style_noHeaders = style.tex(var.title = "", fixef.title = "", stats.title = " ")


kable( etable( reg1 , reg2 , reg3 , reg4 , reg5 ,
        title = 'Average Property Prices for Properties on Sale in Budapest',
        dict = varname_report,
       #drop = vars_omit ,
        #group = groupConf ,
        se.below = T,
        coefstat = 'se',
        fitstat = c('n','r2'),
        se.row = F,
        depvar = F ) ,
        col.names = c('(1)','(2)','(3)','(4)','(5)'),
       "latex", booktabs = TRUE,  position = "H",
       caption = 'Models to uncover relation between Size of Properties and Price of Properties') %>% kable_styling(latex_options = c("hold_position","scale_down"))
```





## Robustness check / 'Heterogeneity analysis'


TODO


## Conclusion
TODO
HERE COMES WHAT WE HAVE LEARNED AND WHAT WOULD STRENGHTEN AND WEAKEN OUR ANALYSIS.

## Appendix
TODO
Here comes all the results which are referenced and not essential for understanding the MAIN results.
```{r}

```

